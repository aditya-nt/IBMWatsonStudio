{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "Welcome to exercise one of \u201cApache Spark for Scalable Machine Learning on BigData\u201d. In this exercise you\u2019ll apply the basics of functional and parallel programming. \n\nLet\u2019s start with a simple example. Let\u2019s consider you have a list of integers.\n\nLet\u2019s find out what the size of this list is.\n\nNote that we already provide an RDD object, so please have a look at the RDD API in order to find out what function to use:\nhttps://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD\n\nThe following link contains additional documentation:\nhttps://spark.apache.org/docs/latest/rdd-programming-guide.html\n\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install --upgrade pip",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Collecting pip\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/0c/d01aa759fdc501a58f431eb594a17495f15b88da142ce14b5845662c13f3/pip-20.0.2-py2.py3-none-any.whl (1.4MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.4MB 7.3MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: pip\n  Found existing installation: pip 19.1.1\n    Uninstalling pip-19.1.1:\n      Successfully uninstalled pip-19.1.1\nSuccessfully installed pip-20.0.2\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "if not ('sc' in locals() or 'sc' in globals()):\n    print('It seems you are note running in a IBM Watson Studio Apache Spark Notebook. You might be running in a IBM Watson Studio Default Runtime or outside IBM Waston Studio. Therefore installing local Apache Spark environment for you. Please do not use in Production')\n    \n    from pip import main\n    main(['install', 'pyspark==2.4.5'])\n    \n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\n\n    sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n    \n    spark = SparkSession \\\n        .builder \\\n        .getOrCreate()",
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "It seems you are note running in a IBM Watson Studio Apache Spark Notebook. You might be running in a IBM Watson Studio Default Runtime or outside IBM Waston Studio. Therefore installing local Apache Spark environment for you. Please do not use in Production\n",
                    "name": "stdout"
                },
                {
                    "output_type": "stream",
                    "text": "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\nPlease see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\nTo avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
                    "name": "stderr"
                },
                {
                    "output_type": "stream",
                    "text": "Collecting pyspark==2.4.5\n  Downloading pyspark-2.4.5.tar.gz (217.8 MB)\nCollecting py4j==0.10.7\n  Downloading py4j-0.10.7-py2.py3-none-any.whl (197 kB)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py): started\n  Building wheel for pyspark (setup.py): finished with status 'done'\n  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257928 sha256=4f8cd007c3bc601548b1b185d53f1e72d56b8d51e19ff1c3baef3803862fac56\n  Stored in directory: /home/dsxuser/.cache/pip/wheels/84/30/e3/c51c5cd0229631e662d29d7b578a3e5949a4c8db033ffb70aa\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.7 pyspark-2.4.5\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "gt50 = lambda i: i > 50",
            "execution_count": 10,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rdd = sc.parallelize(range(100))",
            "execution_count": 11,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# please replace $$ with the correct characters\nrdd.count()",
            "execution_count": 12,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 12,
                    "data": {
                        "text/plain": "100"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You should see \"100\" as answer. Now we want to know the sum of all elements. Please again, have a look at the API documentation and complete the code below in order to get the sum."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rdd.sum()",
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 13,
                    "data": {
                        "text/plain": "4950"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "You should get \"4950\" as answer."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from random import shuffle",
            "execution_count": 14,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rdd.filter(gt50).collect()",
            "execution_count": 15,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 15,
                    "data": {
                        "text/plain": "[51,\n 52,\n 53,\n 54,\n 55,\n 56,\n 57,\n 58,\n 59,\n 60,\n 61,\n 62,\n 63,\n 64,\n 65,\n 66,\n 67,\n 68,\n 69,\n 70,\n 71,\n 72,\n 73,\n 74,\n 75,\n 76,\n 77,\n 78,\n 79,\n 80,\n 81,\n 82,\n 83,\n 84,\n 85,\n 86,\n 87,\n 88,\n 89,\n 90,\n 91,\n 92,\n 93,\n 94,\n 95,\n 96,\n 97,\n 98,\n 99]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rdd.filter(lambda x : x > 50).collect()",
            "execution_count": 16,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 16,
                    "data": {
                        "text/plain": "[51,\n 52,\n 53,\n 54,\n 55,\n 56,\n 57,\n 58,\n 59,\n 60,\n 61,\n 62,\n 63,\n 64,\n 65,\n 66,\n 67,\n 68,\n 69,\n 70,\n 71,\n 72,\n 73,\n 74,\n 75,\n 76,\n 77,\n 78,\n 79,\n 80,\n 81,\n 82,\n 83,\n 84,\n 85,\n 86,\n 87,\n 88,\n 89,\n 90,\n 91,\n 92,\n 93,\n 94,\n 95,\n 96,\n 97,\n 98,\n 99]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "rdd.filter(lambda x : x > 50).filter(lambda x: x < 75).collect()",
            "execution_count": 23,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 23,
                    "data": {
                        "text/plain": "[51,\n 52,\n 53,\n 54,\n 55,\n 56,\n 57,\n 58,\n 59,\n 60,\n 61,\n 62,\n 63,\n 64,\n 65,\n 66,\n 67,\n 68,\n 69,\n 70,\n 71,\n 72,\n 73,\n 74]"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}